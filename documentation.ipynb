{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The objective of this repository is to build a machine learning model that is capable of making predictions on the movement of asset prices. Once a model is made that is of a high enough standard, it will be deployed into production to make predictions on an ongoing basis. The purpose of this documentation is to compliment the code and provide a different way of understanding the nature of the project. It will also provide me with a summary of what I have tried and am yet to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My eventual aim is to do the above for several assets in order to increase the opportunity for the model(s) to provide value and use them in tandem to exploit the different movements of different assets.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main tool I will be using is Databricks. It has a few features which I think will be useful for me including:\n",
    "- Parallel Computing: I will be learning to use pyspark (Python API on Spark computing) in order to distribute my operations between nodes, allowing for more efficient querying and transforming on the data, which could be fairly large considering how far back I would like to go\n",
    "- AutoML: I would take advantage of the AutoML offering in order to build baseline models for each dataset I use, giving me a fast way of knowing what the predictive power of my data might be\n",
    "- MLflow: I will be leaning heavily on MLflow in order to track experiments and different models I build so that I know what direction to go in and know whether a model I built is the best yet; it will also play a large role when it comes to deployment\n",
    "\n",
    "I am also hoping to utilise Numba, a Python library (built into Databricks ML Runtime) that is capable of making my code more efficient when it comes to executing. It does this by making full use of the NVIDIA GPU attached to the cluster to distribute work efficiently between threads. \n",
    "\n",
    "Another tool I am interested in is testing for my code. I want to implement testing into my notebook so that I can quickly see where my code isn't outputting the result I expected. This is important as mistakes in price predictions can be costly if acted upon and so robustness and correctness of my code is of high importance.\n",
    "\n",
    "I will consistently be using Git as I complete this project. This will give me a chance to version my notebooks and return to previous should I make a mistake, it will allow others to contribute to the repository if it gets that far and it allows me to show my work publicly through my GitHub account.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, I am creating a Spark Session. This will allow me to utilise the computing architecture of Apache Spark, for example by creating dataframes that can be parallelized for operations, transformations and querying. I am also creating  a database 'aaronxwalker' in order to store my curated data and different versions of data.\n",
    "\n",
    "I am using the Python package yfinance to import price history for Apple. This is because it has a sufficient amount of liquidity to trade and data is readily available. To begin with I am getting data from the start of 2015 till now; this will hopefully provide me with enough meaningful data to build some complex models but also being slightly faster than working on more data and I do have the option to go back for more should I need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target - simple returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My target column is the percentage change of the stock price from the close of day t-1 to the close of day t. I have chosen Close because it gives me a chance to bet on the outcome whilst the market is open, whereas betting on the Open means I can't take advantage of a difference in price overnight as the market is shut. The reason I am using percentage change is that it is indifferent to the price of the stock, which vary among stocks but has no meaning. It also is likely to have a recognisable distribution which might come in useful to certain parametric models later on.\n",
    "\n",
    "I will also look at the performance of predicting on log returns, which supposedly have even more of a normal distribution and might strengthen the results of some models. It also has an additive property meaning I can simply add the log returns of several days to get the cumulative returns for those days. However, I have yet to test this target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, I am using the features that are provided by yfinance. This includes basic time-series information on the stock price, including open, high, low, volume, dividends and stock splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagging Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important not to use features that we won't have access to in time for inference. For example, we don't want to use the volume for day t to make predictions on the target for day t as the volume won't be accessable till the end of the day, by which time it will be too late to make a prediction. To this end, I have decided to lag each of the above features by a day so that I can use the previous days information to make inferences for the next day.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do all my preliminary work on the data, I used a Pandas dataframe to apply: creating target variable, shifting features, reset index (spark dataframes don't have an index) and renaming columns to be lower case and have no spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Pandas to split my data up. As I'm working with time-series data here, I can't do a random split as the test data would be able to learn from future prices to make predictions, known as data leakage. Therefore, I am splitting up my data by time, with the first 70% used to train my model, the next 15% used to validate the models results and the final 15% used to evaluate the performance of my model on unseen data. I have gone for a 70-15-15 split as I only have a few thousand datapoints and so I want to give enough data to val and test to get robust results. For each row of my validation and test, I plan to use all of the data available up until the day previous to each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then converted my train, val and test dataframes to Spark dataframes which can utilise parallelization. I saved these dataframes as 3 seperate tables in my database under the names `aapl_returns_{train/val/test}` and updated each of their comments box with a little description about the data in the table such as the target, the features and the intended use for the data. As I grow each particular dataset I can overwrite the table, creating a new version that might be for testing or the best data I have yet. This also allows me to rollback a table if I find the newest version isn't beneficial to my model.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a baseline model to get some sort of idea of how much predictive power our features have, I'll be using AutoML to quickly understand my starting point. AutoML on Databricks is going to provide me with a fast way to test different models and see which performs best. It also allows me to use this model to make predictions, see a notebook of the model built and EDA that it done and look at various metrics to see how predictive the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the configuration for AutoML, I'm selecting Forecasting as the ML problem type as I'm working with time-series data and wanting to predict the future, I'm using my train dataset as this is what AutoML is using to train the models, I select the column that is my date column as this is the only feature it uses in Forecasting and I'm predicting 100 steps into the future. I'm hoping that I will be able to see how the robustness of the model varies predicting several steps forwards which should give me an idea on how often I will need to retrain the model with updated data.\n",
    "\n",
    "For evaluation metric, I have selected SMAPE as that's considered a good metric for time-series analysis. Regardless of which I select, all pre-determined metric choices are evaluated when modelling is done and so I will have access to all of their results. There are only 2 forecasting models available (ARIMA and Prophet) and so I will be selecting both for AutoML to try. I can also enter Country Holidays so I have selected US as I am working with a US company. I have selected 20 minutes timeout as it is the end of the day and I don't want to be waiting too long, its only an initial insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this AutoML experiment, I am creating a new experiment named `aapl_returns_forecasting_automl`. In this experiment I will record runs that are done by AutoML and update their versions as each run improves with data or modelling. I also add a description for my experiment to explain what runs in the experiment do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So AutoML was only able to train Prophet models as I hadn't preprocessed the data to have the desired frequency but this isn't an issue as I can still get a rough benchmark from Prophet. It trained 4 different Prophet models in about 8 minutes and has evaluated them on some hidden validation set using SMAPE; I expect I can see the validation set used by going into the provided data exploration notebook or notebook for best model. All of these runs are saved in my experiment I created as well as the notebooks mentioned, the prediction results for the best model (100 steps ahead) and the metrics for all of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going into the run of the best model (based on val_smape), I have access to the notebook to see what was done to the data, I can get the results for the future forecasting and make predictions on new dates. I'm importing the predictions for the next 100 steps/days (confusingly this is now testing on my validation set; autoML made its own validation set from the train set) and visualising them using the UI visualisation tool for Spark DataFrames. We see a small upwards trend in returns over the 100 days which could be a meaningful feature of the data to return to later on, and a weekly seasonality to the predictions. Upon further inspection, autoML doesn't provide an option and will make weekend predictions which are where the peaks of the weekly seasonality occur, so these can be discounted. **I need to check if these skew the results upon being removed or for some reason are abnormal returns that autoML has decided to create.**\n",
    "\n",
    "**However when we plot our predicted returns vs our actual returns, on the whole we can see that the variance in daily returns is almost non-existant and the best the Prophet model can do is essentially predict the same thing every day, the mean of the returns over the given time period.**\n",
    "\n",
    "Clearly, past returns are not going to be enough to predict future returns and I'm going to need to utilise more data.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I start exploring my data, I need to curate my initial dataset. For this, I want to use a diverse range of indicators that will translate well to predictive power for more than just Apple's returns so that I can utilise the Feature Store within Databricks to easily build out more models later on. The data I want to collect is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### macroeconomic\n",
    "\n",
    "**historical inflation**\n",
    "\n",
    "I think inflation is likely one of the more impactful on stock prices, however maybe not on tech stocks as I don't believe they are very resistant to inflation (sitting on cash, raw material costs go up etc) but I also think the impact of inflation on returns will change with time\n",
    "\n",
    "**unemployment rate**\n",
    "\n",
    "I believe people will take money out of more speculative assets as unemployment goes up as they become risk averse, however I believe AAPL will be viewed as a less speculative asset over time meaning it's effect could diminish\n",
    "\n",
    "**GDP**\n",
    "\n",
    "shows how the economy is contracting/expanding, could do binary recession or not variable\n",
    "\n",
    "**interest rate**\n",
    "\n",
    "higher rates mean people are less interested in risky assets but also allows tech companies to earn on their cash piles, interesting to work out real interest rate as well\n",
    "\n",
    "**exchange rates**\n",
    "\n",
    "depending on the company can affect costs and sales e.g. buying from china or selling mainly in US vs global\n",
    "\n",
    "**fixed rate mortgage average**\n",
    "\n",
    "tied to Financial crisis, I think homeowners who get high mortgages want to play it safe. Could see relationship between different mortgage terms to predict returns\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stock market data\n",
    "\n",
    "**FAANG prices**\n",
    "\n",
    "Tech stocks have recently been important in predicting overall market but especially each other and should think about survivorship bias here\n",
    "\n",
    "**S&P500, NASDAQ**\n",
    "\n",
    "It's important to look at how a single stock is doing relative to the wider market \n",
    "\n",
    "**International indices**\n",
    "\n",
    "Again, we want to compare to not only the wider US market but global markets to see if there exists a relationship\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- financial data (p.e etc)\n",
    "- commodity prices\n",
    "- datetime features\n",
    "    - historical dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to dive into exploring the data; this will involve looking at the relationships between features I decide on and the target. \n",
    "\n",
    "Ideally, I also want to be able to use a parametric model where I can get some explainability of how it's deciding on it's predictions. For this I am going to look at the distributions of features and target, I am going to look at the conditional distribution of my target dependent on features and I'm interested in understanding how the (conditional) distributions change over time, if at all. This will help guide me on the reliability of a parametric model's results and whether the data satisfies the assumptions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Familiar Methods\n",
    "\n",
    "- how correlated are all of our columns?\n",
    "- mean and variance of interesting variable e.g. returns?\n",
    "- do the values vary by time? should I only be training on a recent subsection of total data?\n",
    "- does the data look consistent in its behaviour over time?\n",
    "- histograms, scatter plots, statistics w.r.t time\n",
    "- group by:\n",
    "    - day, week, month; is there any trend in min, max, var, mean of returns\n",
    "    - macro: unemployment, GDP, inflation\n",
    "    - micro: earnings days, competitor earnings days\n",
    "- for histograms, by detrending the price (finding returns/difference instead) we are likely to get a more normal distribution\n",
    "    - if we are still seeing a trend in the data, we are not going to get a normal distribution so do something\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series Data Methods\n",
    "\n",
    "We will check for stationarity in our data beause it's an assumption of models such as linear regression and NNs. If we cannot get our data to be stationary then we will have to resort to using models that don't make any assumptions about the data.\n",
    "\n",
    "- testing the data for stationarity: for all possible lags k, the distribution of yₜ, yₜ₊₁...yₜ₊ₖ does not depend on t\n",
    "    - many time-series models rely on the data being stationary: constant mean and variance over time\n",
    "    - strong seasonal behaviour is the antithesis of stationarity\n",
    "    - check using AR(p) model the coefficients for p lags and if any more than or equal to one i think thats bad and implies\n",
    "    non-stationarity\n",
    "    - Use Augmented Dicky-Fuller (ADF) test to assert whether stationary\n",
    "    - log or root transformations can fix non-constant variance\n",
    "    - differencing can fix a trend\n",
    "    - if non-stationary then fix\n",
    "- for some models, they assume normality in the distribution of input variables and predictor variable\n",
    "    - use transformation to achieve this result but understand why doing it\n",
    "- applying rolling functions\n",
    "    - rolling windows:\n",
    "        - used for either compressing/downsampling or smoothing\n",
    "    - expanding windows:\n",
    "        - makes sense in a stable process e.g. where the current price is compared to long term mean as a feature\n",
    "- identifying self-correlation\n",
    "    - value t correlated with value t-k\n",
    "    - autocorrelation assumes a linear relationship but this may not be the case and there can still be a deterministic pattern \n",
    "    - a statistical rule for determining a non-zero ACF estimate is given by a 'critical region' with bounds at +/-1.96*sqrt(n), relying on a large enough sample size and finite variance\n",
    "    - if detecting more than one self-correlation, important to measure partial autocorrelation rather than redundant correlations\n",
    "    - if we break up our target into trend, season, noise etc and want to do autocorrelations on each:\n",
    "        - sum of periodic series has AFC of sum of individual ACFs\n",
    "        - not the same with PACFs\n",
    "    - spurious correlations:\n",
    "        - be wary of the fact we often have correlations that arent causation, when two sets of data both have a trend, theres a good chance theyre going to have a strong correlation\n",
    "        - seasonality is often proxied by two completely unrelated things that both are seasonal\n",
    "        - instead we are looking for cointegration: a real relationship between two time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful viz\n",
    "\n",
    "2-D plots when you want to see how a variable impacts the target whilst taking into account something else through color or shape or size\n",
    "e.g. how the per-season trend in returns changes by plotting mean and variance of returns over time grouped by season or day etc\n",
    "e.g. how the importance of a variable has changed over time looking at rolling corr\n",
    "if you have a feature you want to use as a colour or groupby, consider transforming it if very skewed to get an even set of colours\n",
    "\n",
    "when doing a histogram you can color by a second variable but **it is important to note that histograms rely on stationary data**. \n",
    "\n",
    "3-D plots when you want to compare up to 3 features unanimously with the target\n",
    "use plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decompose the long term trend, seasonality and errors from my data, I can use `statsmodels.tsa.seasonal.seasonal_decompose`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas to improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I'm struggling to improve the performance of my model, here are some ideas I plan to try:\n",
    "- increase the number of useful features in my model (gather more external data)\n",
    "- try to split up the data into different periods so that all the data used comes from the same distribution\n",
    "- make a classification model to predict +/- returns for the day\n",
    "- change the time period I am predicting over e.g. predict returns over the next month instead of next day if short term volatility is too difficult to predict\n",
    "    - if this is the case then consider smoothing the price and using that as target\n",
    "        - exp. smoothing should only be used on stationary data so detrend first or use e.g. Holt (Winters) smoothing\n",
    "- explore different models\n",
    "- read a book on time-series\n",
    "- try a different asset\n",
    "- conformal prediction to make confidence interval predictions\n",
    "- can I do PCA on my data (inc target) to see if I can seperate the data into periods where there is a constant relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in missing data: Imputing using a rolling average will reduce variance of your data, bear this in mind when thinking about reliability of model's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0xfD311892F7d1416f8EB463d8Cd675B18890C646f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0xfD311892F7d1416f8EB463d8Cd675B18890C646f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
